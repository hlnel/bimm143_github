---
title: "Class 08: Unsupervised Learning Analysis of Human Breast Cells"
author: "Helen Le (PID:16300695)"
format: pdf
---

> 1. Exploratory Data Analysis

## Data input
The data is supplied on CSV format:

```{r}
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```
The   `row.names` argument in the `read.csv()` function serves to make the ID numbers the row names so they're not included in the table. (think Duke incident)

Remove the diagnosis (column 1) from the data set & create a separate vector for the diagnosis for later use.

```{r}
# Use -1 to remove the first column
wisc.data <- wisc.df[,-1]
# Create a diagnosis vector
diagnosis <- as.factor(wisc.df[,1])

head(wisc.data)
head(diagnosis)
```

**Q1. How many observations are in this dataset?**
```{r}
nrow(wisc.df)
```
There are 569 patients in this data set.

**Q2. How many of the observations have a malignant diagnosis?**
```{r}
table(diagnosis)
```
212 of the observations have a malignant diagnosis.

**Q3. How many variables/features in the data are suffixed with _mean?**
```{r}
# Use `colnames()` to access the column names in the data frame
colnames(wisc.df)

# Figure out which columns contain the suffix & assign it to a vector
suffix <- grep(pattern="_mean", x=colnames(wisc.df))

# Find the length of the vector for the total number
length((suffix))
```
10 of the variables are suffixed with "_mean".

> 2. Principal Component Analysis

We need to scale our input data before PCA as some of the columns are measured in terms of very different units with different means and different variances. The upshot here i we set `scale=TRUE` argument to `prcomp()`.

The scale argument scales the variables by their standard deviations, which is important for variables on different scales, making their overall scales more comparable for PCA.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

**Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?**
0.4427 of the original variance is captured by PC1.

**Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**
The first 3 PCs are required to describe at least 70% of the original variance of the data.

**Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**
The first 7 PCs are required to describe at least 90% of the original variance of the data.

```{r}
biplot(wisc.pr)
```
**Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?**
The variables listed in this plot is all over the place, making it difficult to understand.

```{r}
head(wisc.pr$x)
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, pch=16)
```

**Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?**

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
In this plot between PC1 and PC3, it's harder to determine a hard cut-off to separate the two subgroups.

Let's view this using ggplot:
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance Explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
This is another way to visualize the amount of variance between the difference principal components.

```{r}
## ggplot based graph
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
Opened a lot of package but this is another way to visualize the variance.

**Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**
```{r}
wisc.pr$rotation[,1]
wisc.pr$rotation["concave.points_mean",1]
```
The component of the loading vector for the feature concave.points_mean is -0.2608538.

**Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**
```{r}
summary(wisc.pr)
```
5 is the minimum number of PCs required to explain 80% of the variance of the data.

> 3. Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

# Calculate the distance between the pairs in the newly scaled dataset
data.dist <- dist(data.scaled)

# Creat & plot a hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```

**Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
The height at which the clustering model has 4 clusters is `h=19`.

## Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

# Compare cluster membership to the actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```
Here, we see that cluster 1 mainly consists of malignant cells while cluster 3 consists of benign cells.

**Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?**
Yes, depending on the data, different number of clusters may be suitable for matching the cluster vs. diagnoses. In this case, having a higher amount of clusters may be beneficial as it could produce results that are closer to the true spread of diagnoses in the patients. The incorporation of more clusters may help in making the clusters more homogeneous and better separate the subgroups.

**Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**
For the same data.dist dataset, the `ward.D2` method gives my favorite results since it accounts for the within cluster variance, and produces clusters of roughly equal sizes.

> 5. Combining Methods

This approach will take not original data but our PCA results and work with them.

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
grps
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```
We see some overlap between the red and black data points. This is due to the 2D visualization of 3D data-- there's another dimension we're overlooking.

```{r}
table(diagnosis)
```

To compare with the plot of diagnoses:
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
table(diagnosis, grps)
```
There's 24 false positives (benign in group 1) & 33 false negatives (malignant in group 2).

To make the plots match in color, the factors can be reordered:
```{r}
g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)

# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

Cut hierarchical clustering model-- wisc.pr.hclust.two-- into 2 clusters:
```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust.two <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust.two, k=2)

# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

**Q15. How well does the newly created model with four clusters separate out the two diagnoses?**
The newly created model does similarly with the four clusters in separating out the two diagnoses. There are more true positives and less false positives. Unfortunately, there are less true negatives and more false negatives.